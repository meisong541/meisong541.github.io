<!DOCTYPE html>
<html lang="en"><head>
  <meta charset="utf-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <meta name="viewport" content="width=device-width, initial-scale=1"><!-- Begin Jekyll SEO tag v2.6.1 -->
<title>Statistical physics and random matrix theory: Replica method (I) | Entropic Flow</title>
<meta name="generator" content="Jekyll v3.8.6" />
<meta property="og:title" content="Statistical physics and random matrix theory: Replica method (I)" />
<meta name="author" content="Song Mei" />
<meta property="og:locale" content="en_US" />
<meta name="description" content="A research blog on machine learning and statistics." />
<meta property="og:description" content="A research blog on machine learning and statistics." />
<link rel="canonical" href="http://localhost:4000/jekyll/update/2019/08/02/Replica_method_1.html" />
<meta property="og:url" content="http://localhost:4000/jekyll/update/2019/08/02/Replica_method_1.html" />
<meta property="og:site_name" content="Entropic Flow" />
<meta property="og:type" content="article" />
<meta property="article:published_time" content="2019-08-02T00:09:06-07:00" />
<script type="application/ld+json">
{"description":"A research blog on machine learning and statistics.","author":{"@type":"Person","name":"Song Mei"},"@type":"BlogPosting","url":"http://localhost:4000/jekyll/update/2019/08/02/Replica_method_1.html","headline":"Statistical physics and random matrix theory: Replica method (I)","dateModified":"2019-08-02T00:09:06-07:00","datePublished":"2019-08-02T00:09:06-07:00","mainEntityOfPage":{"@type":"WebPage","@id":"http://localhost:4000/jekyll/update/2019/08/02/Replica_method_1.html"},"@context":"https://schema.org"}</script>
<!-- End Jekyll SEO tag -->
<link rel="stylesheet" href="/assets/css/style.css"><link type="application/atom+xml" rel="alternate" href="http://localhost:4000/feed.xml" title="Entropic Flow" /><link rel="canonical" href="http://localhost:4000/jekyll/update/2019/08/02/Replica_method_1.html">
  <link rel="alternate" type="application/rss+xml" title="Entropic Flow" href="/feed.xml">
  <link rel="shortcut icon" type="image/x-icon" href="/assets/favicon.ico?">

  

  <link href="https://maxcdn.bootstrapcdn.com/font-awesome/4.7.0/css/font-awesome.min.css" rel="stylesheet" integrity="sha384-wvfXpqpZZVQGK6TAh5PVlGOfQNHSoD2xbE+QkPxCAFlNEevoEH3Sl0sibVcOQVnN" crossorigin="anonymous">


  <script type="text/javascript" async
  src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js?config=TeX-MML-AM_CHTML" async>
</script>
</head>
<body><header class="site-header">

  <div class="wrapper"><a class="site-title" rel="author" href="/">Entropic Flow</a><nav class="site-nav">
        <input type="checkbox" id="nav-trigger" class="nav-trigger" />
        <label for="nav-trigger">
          <span class="menu-icon">
            <svg viewBox="0 0 18 15" width="18px" height="15px">
              <path d="M18,1.484c0,0.82-0.665,1.484-1.484,1.484H1.484C0.665,2.969,0,2.304,0,1.484l0,0C0,0.665,0.665,0,1.484,0 h15.032C17.335,0,18,0.665,18,1.484L18,1.484z M18,7.516C18,8.335,17.335,9,16.516,9H1.484C0.665,9,0,8.335,0,7.516l0,0 c0-0.82,0.665-1.484,1.484-1.484h15.032C17.335,6.031,18,6.696,18,7.516L18,7.516z M18,13.516C18,14.335,17.335,15,16.516,15H1.484 C0.665,15,0,14.335,0,13.516l0,0c0-0.82,0.665-1.483,1.484-1.483h15.032C17.335,12.031,18,12.695,18,13.516L18,13.516z"/>
            </svg>
          </span>
        </label>

        <div class="trigger"><a class="page-link" href="/about/">About</a><a href="https://web.stanford.edu/~songmei">Homepage</a>
        </div>
      </nav></div>
</header><main class="page-content" aria-label="Content">
      <div class="wrapper">
        


<article class="post" itemscope itemtype="http://schema.org/BlogPosting">

  <header class="post-header">
    <h1 class="post-title" itemprop="name headline">Statistical physics and random matrix theory: Replica method (I)</h1>
    <p class="post-meta">
      <time datetime="2019-08-02T00:09:06-07:00" itemprop="datePublished">
        
        Aug 2, 2019
      </time>
       |
      
      
      </p>
  </header>

 <div class="post-content" itemprop="articleBody">
    
   <p><script type="math/tex">\def\N{\mathbb N}</script>
<script type="math/tex">\def\P{\mathbb P}</script>
<script type="math/tex">\def\bi{\boldsymbol i}</script>
<script type="math/tex">\def\bG{\boldsymbol G}</script>
<script type="math/tex">\def\bsigma{\boldsymbol \sigma}</script>
<script type="math/tex">\def\bv{\boldsymbol v}</script>
<script type="math/tex">\def\bu{\boldsymbol u}</script>
<script type="math/tex">\def\sT{\mathsf T}</script>
<script type="math/tex">\def\bW{\boldsymbol W}</script>
<script type="math/tex">\def\bA{\boldsymbol A}</script>
<script type="math/tex">\def\R{\mathbb R}</script>
<script type="math/tex">\def\S{\mathbb S}</script>
<script type="math/tex">\def\GOE{\text{GOE}}</script>
<script type="math/tex">\def\|{\Vert}</script>
<script type="math/tex">\def\bx{\boldsymbol x}</script>
<script type="math/tex">\def\cN{\mathcal N}</script>
<script type="math/tex">\def\E{\mathbb E}</script>
<script type="math/tex">\def\de{\text{d}}</script>
<script type="math/tex">\def\vphi{\varphi}</script>
<script type="math/tex">\def\bQ{\boldsymbol Q}</script>
<script type="math/tex">\def\diag{\text{diag}}</script>
<script type="math/tex">\def\bzero{\boldsymbol 0}</script>
<script type="math/tex">\def\id{\mathbf I}</script>
<script type="math/tex">\def\ones{\mathbf 1}</script>
<script type="math/tex">\def\ext{\text{ext}}</script>
<script type="math/tex">\def\|{\Vert}</script>
<script type="math/tex">\def\bLambda{\boldsymbol \Lambda}</script>
<script type="math/tex">\def\const{\text{const}}</script>
<script type="math/tex">\def\Unif{\text{Unif}}</script>
<script type="math/tex">\def\bSigma{\boldsymbol \Sigma}</script></p>

<h2 id="1-introduction">1. Introduction</h2>

<p>In my first several blogs, I will discuss about replica method. Replica method is a non-rigorous but powerful method that originated from statistical physics literature. It has been widely applied in other fields, including coding theory and high dimensional statistics. I used several times this method in my research. It helped me quickly guessing the answers.</p>

<p>In this blog, I will discuss about how to use replica method to calculate the spectral norm of certain random matrices. In the next few blogs, I will discuss about how to use it to calculate the spectral density (Stieltjes transforms) of certain random matrices.</p>

<!-- In my research, I encountered many interesting problems that is related to random matrices. There are many rigorous and powerful tools dealing with random matrices, like concentration inequalities, Stieltjes transform, leave-one-out method, etc. Besides these methods, there is one tool that I find it to be extremely powerful. It is called the replica method, which is a non-rigorous method originated from statistical physics. I used several times this tool in my research, and it helped me a lot to "guess" the answer quickly. I gave two talks about this method, respectively in Andrea's and Tengyu's group meetings. Now I think it is the time to summarize my talks and present it to the large audiences. -->
<!-- 
Replica method was used to calculate the free energy associated to some random Hamiltonians in statistical physics. Its first well-known application is calculating the free energy of Sherrington-Kirkpatrick model. Beyond the physicists' community, replica method is a very powerful tool studying random matrices. Its applications includes coding theory, compressed sensing, and high dimensional statistical inference problems. In this blog, I will discuss about how to use replica method to predict the spectral norm of spiked GOE matrix.

For the references for replica method, I suggest people reading: .....
 -->

<h2 id="2-a-motivating-example-spiked-goe-matrix">2. A motivating example: spiked GOE matrix</h2>

<p>We consider a symmetric random matrix <script type="math/tex">\bA \in \R^{n \times n}</script>, 
\[
\bA= \lambda \bu \bu^\sT + \bW,
\]
where <script type="math/tex">\lambda \ge 0</script> is the signal to noise ratio, <script type="math/tex">\bu \in \S^{n-1} \equiv \{ \bx \in \R^n: \| \bx \|_2 = 1 \}</script> is a spike vector, and <script type="math/tex">\bW \sim \GOE(n)</script>: that means, <script type="math/tex">\bW \in \R^{n \times n}</script> is a symmetric matrix, <script type="math/tex">W_{ij} \sim \cN(0, 1/n)</script> for <script type="math/tex">% <![CDATA[
1 \le i < j \le n %]]></script>, and <script type="math/tex">W_{ii} \sim \cN(0, 2/n)</script> for <script type="math/tex">1 \le i \le n</script>.</p>

<p>We are interested in the largest eigenvalue and the corresponding eigenvector of <script type="math/tex">\bA</script>, which are respectively denoted as <script type="math/tex">\lambda_{\max}(\bA)</script> and <script type="math/tex">\bv_{\max}(\bA)</script>. The result below is called BBP phase transition phenomenon (see <a rel="nofollow" target="_blank" href="https://projecteuclid.org/euclid.aop/1127395869">this paper</a> for spiked Wishart matrix and <a rel="nofollow" target="_blank" href="https://link.springer.com/article/10.1007/s00440-005-0466-z">this paper</a> for spiked Wigner matrix).</p>

<ul>
  <li>
    <p>The largest eigenvalue <script type="math/tex">\lambda_{\max}(\bA)</script>: <br />
For <script type="math/tex">0 \le \lambda \le 1</script>, we have
\[
\lim_{n \to \infty} \E[\lambda_{\max} ({\bA})] = 2;
\]
For <script type="math/tex">\lambda > 1</script>, we have
\[
\lim_{n \to \infty} \E[\lambda_{\max} ({\bA})] = \lambda + 1/\lambda.
\]</p>
  </li>
  <li>
    <p>The correlation of top eigenvector <script type="math/tex">\bv_{\max}(\bA)</script> with <script type="math/tex">\bu</script>: <br />
For <script type="math/tex">0 \le \lambda \le 1</script>, we have
\[
\lim_{n \to \infty} \E[\langle \bv_{\max}(\bA), \bu \rangle^2] = 0;
\]
For <script type="math/tex">\lambda > 1</script>, we have
\[
\lim_{n \to \infty} \mathbb{E}[\langle {\boldsymbol v}_{\max}({\boldsymbol A}), {\boldsymbol u}\rangle^2] = 1 - 1/\lambda^2.
\]</p>
  </li>
</ul>

<p>In the rest of this blog, we will derive this result using replica method.</p>

<h2 id="3-tricks-in-statistical-physics">3. Tricks in statistical physics</h2>

<h3 id="31-the-free-energy-trick">3.1. The free energy trick</h3>

<p>Let <script type="math/tex">H: \Omega \to \R</script> and <script type="math/tex">f: \Omega \to \R</script> be two functions. We would like to calculate the following quantities
\[
\begin{aligned}
&amp;\max_{\bsigma \in \Omega} H(\bsigma),<br />
&amp;f(\arg \max_{\bsigma} H(\bsigma)).
\end{aligned}
\]</p>

<p>The free energy trick works as follows. Define a Gibbs measure
\[
\mu_{\beta, h}(\de \bsigma) = \frac{1}{Z(\beta, h)} \exp\{ \beta H(\bsigma) + h f(\bsigma)\} \nu_0(\de \bsigma),
\]
where <script type="math/tex">Z(\beta, h)</script> denotes the normalizing constant
\[
\begin{align}\tag{Def Z} \label{eqn:def_Z}
Z(\beta, h) = \int_{\Omega} \exp\{ \beta H(\bsigma) + h f(\bsigma)\} \nu_0(\de \bsigma),
\end{align}
\]
and <script type="math/tex">\nu_0</script> denotes a reference measure on <script type="math/tex">\Omega</script>. In physics, <script type="math/tex">\beta</script> stands for the inverse temperature, and <script type="math/tex">h</script> stands for the strength of external field. Define the free energy function by 
\[
\Psi(\beta, h) = \log Z(\beta, h). 
\]
The following lemma shows that, the free energy function <script type="math/tex">\Psi</script> can generate many interesting quantities, including <script type="math/tex">\max_{\bsigma \in \Omega} H(\bsigma)</script> and <script type="math/tex">f(\arg \max_{\bsigma} H(\bsigma))</script>.</p>

<div class="lemma">
Denote the ensemble average operator <script type="math/tex">\langle \cdot \rangle</script> by
\[
\langle g \rangle_{\beta, h} = \int_\Omega g(\bsigma) \mu_{\beta, h}(\de \bsigma).
\]
Then we have
\begin{align}\tag{1}\label{eq:1}
\partial_\beta \Psi(\beta, h) =&amp; \langle H \rangle_{\beta, h},\\
\partial_h \Psi(\beta, h) =&amp; \langle f \rangle_{\beta, h},\\
\end{align}
and
\begin{align}\tag{2}\label{eq:2}
\lim_{\beta \to \infty} \partial_\beta \Psi(\beta, h) =&amp; \max_{\bsigma \in \Omega} H(\bsigma),\\
\lim_{\beta \to \infty} \partial_h \Psi(\beta, h) =&amp; f(\arg\max_{\bsigma \in \Omega} H(\bsigma) ).
\end{align}
</div>
<p>Equation \eqref{eq:1} follows from basic calculus of exponential family. The intuition for Equation \eqref{eq:2} gives the following: for <script type="math/tex">h = 0</script> and large <script type="math/tex">\beta</script>, the measure <script type="math/tex">\mu_{\beta, 0}</script> concentrates at the maxima of <script type="math/tex">H</script>.</p>

<!-- ### 3.2. Free energy trick for spiked GOE model -->

<p>In the spiked GOE model, the Hamiltonian associated with the random matrix <script type="math/tex">\bA</script> is defined as
\[
H_{n, \lambda}(\bsigma) = \langle \bsigma, \bA \bsigma \rangle = \lambda \langle \bu, \bsigma\rangle^2 + \langle \bsigma, \bW \bsigma\rangle. 
\]
Denote the (random) partition function associated with the Hamiltonian <script type="math/tex">H_{n, \lambda}</script> by
\[
Z(\beta, \lambda, n) = \int_{\S^{n-1}} \exp\{ \beta n H_{n, \lambda}( \bsigma ) \} \nu_0(\de \bsigma).
\]
Here <script type="math/tex">\nu_0</script> is the uniform probability measure on <script type="math/tex">\S^{n-1}</script>, and the <script type="math/tex">n</script> factor in the exponent serves for proper normalization.</p>

<p>Define 
\[
\begin{align}\tag{3}\label{eq:3}
\vphi_n(\lambda) =&amp; \lim_{\beta \to \infty} \frac{1}{\beta n}\E[\log Z(\beta, \lambda, n)],\\
\vphi(\lambda) =&amp; \lim_{n\to \infty} \vphi_n(\lambda) = \lim_{n \to \infty} \lim_{\beta \to \infty} \frac{1}{\beta n}\E[\log Z(\beta, \lambda, n)]. 
\end{align}
\]
Due to the lemma below, in order to get the asymptotics of <script type="math/tex">\E[\lambda_{\max}(\bA)]</script> and <script type="math/tex">\E[\langle \bv_{\max}(\bA), \bu\rangle^2]</script>, we just need to calculate <script type="math/tex">\vphi(\lambda)</script>.</p>

<div class="lemma">
For <script type="math/tex">\bA = \lambda \bu \bu^\sT + \bW \in \R^{n \times n}</script>, we have
\[
\begin{aligned}
\vphi_n(\lambda) =&amp; \E[\sup_{\bsigma} H_{n, \lambda}(\bsigma)]= \E[\lambda_{\max} (\bA)],\\
\vphi_n'(\lambda) =&amp; \E[\langle \bv_{\max}(\bA), \bu \rangle^2]. 
\end{aligned}
\]
</div>
<p>We leave the proof of this lemma to readers.</p>

<h3 id="32-the-replica-trick">3.2. The replica trick</h3>

<p>The difficulty of calculating <script type="math/tex">\vphi(\lambda)</script> originated from the expectation of <script type="math/tex">\log Z</script> as in Eq. \eqref{eq:3}. The replica trick can convert the problem of calculating the expectation of <script type="math/tex">\log Z</script> into calculating the moments of <script type="math/tex">Z</script>. Calculating the moments of <script type="math/tex">Z</script> is an easier problem.</p>

<p>In mathematics, the replica trick gives the following formula
\[\tag{4} \label{eq:4}
\E[\log Z] = \lim_{k \to 0} \frac{1}{k}\log \E[Z^k ]. 
\]
Here is a simple proof of this equality. 
\[
\begin{aligned}
\E[\log Z] =&amp; \E[(\log Z^k) / k] = \lim_{k \to 0} \E [ \log(1 + Z^k - 1) / k] = \lim_{k \to 0} \E [ ( Z^k - 1 ) / k ] \\
=&amp;\lim_{k \to 0} (\E [ Z^k] - 1)/k = \lim_{k \to 0} \frac{1}{k} \log (1 + \E[Z^k] - 1) = \lim_{k \to 0} \frac{1}{k}\log \E[Z^k ]. 
\end{aligned}
\]</p>

<!-- ### 3.4. The replica trick for spiked GOE model -->

<p>Using the definition of <script type="math/tex">\vphi</script> in Eq. \eqref{eq:3} and the replica trick in Eq. \eqref{eq:4}, we can rewrite <script type="math/tex">\vphi</script> as
\[
\begin{aligned}
\vphi(\lambda) \equiv&amp; \lim_{n\to \infty} \lim_{\beta \to \infty} \frac{1}{\beta n}\E[\log Z(\beta, \lambda, n)] = \lim_{n \to \infty}   \lim_{\beta \to \infty} \lim_{k \to 0}\frac{1}{\beta k n}\log \E[Z(\beta, \lambda, n)^k]. \\
\end{aligned}
\]
With some heuristic change of limit (which we will not justify), we get
\[
\vphi(\lambda) \stackrel{\cdot}{=} \lim_{\beta \to \infty}  \lim_{k \to 0} \lim_{n \to \infty} \frac{1}{\beta k n}\log \E[Z(\beta, \lambda, n)^k]. 
\]
There are three limits in the right hand side of the above equation. Define
\[
\begin{align}\tag{Def S} \label{eqn:defS}
S(k, \beta, \lambda) = \lim_{n \to \infty} \frac{1}{n}\log \E[Z(\beta, \lambda, n)^k],
\end{align}
\]
and
\[
\begin{align}\tag{Def $\Psi$} \label{eqn:defPsi}
\Psi(\beta, \lambda) = \lim_{k \to 0} \frac{1}{k} S(k, \beta, \lambda).
\end{align}
\]
This gives
\[
\begin{align}\tag{Def $\vphi$} \label{eqn:defvphi}
\vphi(\lambda) = \lim_{\beta \to \infty} \frac{1}{\beta}\Psi(\beta, \lambda). 
\end{align}
\]
In the following section, we calculate <script type="math/tex">S</script>, <script type="math/tex">\Psi</script>, and <script type="math/tex">\vphi</script> functions sequentially.</p>

<h2 id="4-the-replica-calculations">4. The replica calculations</h2>

<h3 id="41-the-large-n-limit-moments-calculation">4.1. The large <script type="math/tex">n</script> limit: moments calculation</h3>

<h4 id="411-a-variational-formula-for-s-function">4.1.1. A variational formula for <script type="math/tex">S</script> function</h4>

<p>Recall the definition of <script type="math/tex">S</script> function given in Eq. \eqref{eqn:defS}. We claim the following equality for <script type="math/tex">k \in \N_+</script>
\begin{align}\tag{5}\label{eqn:S_function}
S(k, \beta, \lambda) \equiv \lim_{n \to \infty} \frac{1}{n}\log\E[Z(\beta, \lambda, n)^k]= \sup_{\bQ \in \R^{(k+1) \times (k+1)}, \diag(\bQ) = 1, \bQ \succeq \bzero} {U(\bQ) }, 
\end{align}
where
\begin{align}\tag{6}\label{eqn:U_function}
U(\bQ) = \beta \lambda \sum_{i=1}^k q_{0i}^2 + \beta^2 \sum_{ij = 1}^k q_{ij}^2 + \frac{1}{2} \log(\det(\bQ))
\end{align}
and <script type="math/tex">\bQ \in \R^{(k + 1) \times (k + 1)}</script> is symmetric, with (the index of <script type="math/tex">\bQ</script> starts from <script type="math/tex">0</script> and ends at <script type="math/tex">k</script>)
\[
\begin{align}\tag{Def $\bQ$} \label{eqn:def_Q}
\bQ = \begin{bmatrix}
1&amp; q_{01} &amp; \ldots &amp; q_{0k}\\
q_{01} &amp; 1 &amp; \ldots &amp; q_{1k}\\
\ldots &amp; \ldots &amp; \ldots &amp; \ldots\\
q_{0k} &amp; q_{k1} &amp; \ldots &amp; 1\\
\end{bmatrix}.
\end{align}
\]
Equation \eqref{eqn:S_function} is exact and rigorous, but the computation is involved. By calculating the moments of <script type="math/tex">Z</script> directly (the calculation is given in Section <a href="#sec:derivation">4.1.2</a>), we get
\begin{align}\tag{7} \label{eqn:moments_result}
\E[Z(\beta, \lambda, n)^k] = \int \exp\{ n U(\bQ) + o(n)\} \de \bQ. 
\end{align}
Eq. \eqref{eqn:S_function} follows from Eq. \eqref{eqn:moments_result} and Laplace method. We suggest the readers skipping the derivation of Eq. \eqref{eqn:moments_result} at the first time reading, and continue at Section <a href="#sec:rsa">4.2</a>.</p>

<h4 id="sec:derivation">4.1.2. The replica calculations</h4>

<p>In this section, we derive Eq. \eqref{eqn:moments_result}. At the first time reading, we suggest the readers skipping this section and continue at Section <a href="#sec:rsa">4.2</a>.</p>

<p>Recall the definition of <script type="math/tex">Z</script> given in Eq. \eqref{eqn:def_Z}. The <script type="math/tex">k</script>‘th moments of <script type="math/tex">Z(\beta, \lambda, n)</script> gives 
\[
\begin{aligned}
\E[Z(\beta, \lambda, n)^k] = &amp; \E\Big[\Big(\int_{\S^{n-1}} \exp\{ \beta n H_{n, \lambda}( \bsigma ) \} \nu_0(\de \bsigma)\Big)^k \Big].<br />
\end{aligned}
\]
Here, the expectation <script type="math/tex">\E</script> is taken with respect to the randomness of matrix <script type="math/tex">\bA</script> in <script type="math/tex">H_{n, \lambda}(\bsigma) = \langle \bsigma, \bA \bsigma\rangle</script>. The first trick is to create <script type="math/tex">k</script> replicas of <script type="math/tex">\bsigma</script>, such that we can exchange the expectation with integrals, 
\[
\begin{aligned}
\E[Z(\beta, \lambda, n)^k] =&amp;\E\Big[\Big(\int_{\S^{n-1}} \exp\{ \beta n H_{n, \lambda}( \bsigma ) \} \nu_0(\de \bsigma)\Big)^k \Big] = \E\Big[\int_{(\S^{n-1})^k} \exp\Big\{ \beta n \sum_{i = 1}^k H_{n, \lambda}( \bsigma_i ) \Big\} \prod_{i \in [k]} \nu_0(\de \bsigma_i) \Big]\\
=&amp;\int_{(\S^{n-1})^k} \E\Big[ \exp\Big\{ \beta n \sum_{i = 1}^k  [\lambda \langle \bu, \bsigma_i\rangle^2 + \langle \bsigma_i, \bW \bsigma_i\rangle ] \Big\}  \Big] \prod_{i \in [k]} \nu_0( \de \bsigma_i).
\end{aligned}
\]
Let <script type="math/tex">\bG = (G_{ij})_{i, j \in [n]} \in \R^{n \times n}</script> with <script type="math/tex">G_{ij} \sim \cN(0, 1)</script> independently. Noting that the GOE matrix <script type="math/tex">\bW</script> shares the same distribution with matrix <script type="math/tex">(\bG + \bG^\sT) / \sqrt{2 n}</script>, we have
\[
\begin{aligned}
\E[Z(\beta, \lambda, n)^k] =&amp; \int_{(\S^{n-1})^k} \E_{G_{ij} \sim \cN(0,1)}\Big[ \exp\Big\{ \beta n \lambda \sum_{i = 1}^k  \langle \bu, \bsigma_i\rangle^2 + \beta n \Big\langle  (\bG + \bG^\sT) / \sqrt{2n},  \sum_{i=1}^k \bsigma_i\bsigma_i^\sT \Big\rangle \Big\} \Big] \prod_{i \in [k]} \nu_0(\de \bsigma_i) \\
=&amp; \int_{(\S^{n-1})^k} \exp\Big\{ \beta n \lambda \sum_{i = 1}^k  \langle \bu, \bsigma_i\rangle^2 \Big\} \times \E_{G_{ij} \sim \cN(0,1)}\Big[ \exp\Big\{ \beta \sqrt{2 n} \Big\langle \bG,  \sum_{i=1}^k \bsigma_i\bsigma_i^\sT \Big\rangle \Big\} \Big] \prod_{i \in [k]} \nu_0(\de \bsigma_i). 
\end{aligned}
\]
Using the formula of Gaussian moment generating function <script type="math/tex">\E_{G \sim \cN(0, 1)}[e^{tG}] = e^{t^2/2}</script>, we get
\[
\begin{aligned}
\E[Z(\beta, \lambda, n)^k] =&amp; \int_{(\S^{n-1})^k} \exp\Big\{ \beta n \lambda \sum_{i = 1}^k  \langle \bu, \bsigma_i \rangle^2  + n \beta^2 \Big \Vert \sum_{i=1}^k \bsigma_i \bsigma_i^\sT \Big\Vert_F^2 \Big\}  \prod_{i \in [k]} \nu_0(\de \bsigma_i) \\
=&amp; \int_{(\S^{n-1})^k} \exp\Big\{ \beta n \lambda \sum_{i = 1}^k  \langle \bu, \bsigma_i \rangle^2 + n \beta^2 \sum_{i, j=1}^k\Big\langle  \bsigma_i\bsigma_i^\sT, \bsigma_j \bsigma_j^\sT\Big \rangle \Big\} \prod_{i \in [k]} \nu_0(\de \bsigma_i) \\
=&amp; \int_{(\S^{n-1})^k} \exp\Big\{ \beta n \lambda \sum_{i = 1}^k  \langle \bu, \bsigma_i \rangle^2 + n \beta^2 \sum_{i, j=1}^k \langle \bsigma_i, \bsigma_j \rangle^2 \Big\} \prod_{i \in [k]} \nu_0(\de \bsigma_i). 
\end{aligned}
\]</p>

<p>In the following, we make use a heuristic argument (using Dirac delta function). Note we have
\[
1 = \int \prod_{i=1}^k \delta \Big( \langle \bu,\bsigma_i\rangle - n q_{0i} \Big) \prod_{1 \le i &lt; j\le k} \delta\Big( \langle\bsigma_i, \bsigma_j\rangle - n q_{ij} \Big) \de \bQ
\]
where <script type="math/tex">% <![CDATA[
\de \bQ \equiv \prod_{i \in [k]} \de q_{0i} \prod_{1 \le i < j \le k} \de q_{ij} %]]></script>. Using this equality, we get
\[
\begin{aligned}
\E[Z(\beta, \lambda, n)^k] =&amp;  \int \de \bQ \exp\Big\{\beta n \lambda \sum_{i = 1}^k  q_{0i}^2 + n \beta^2  \sum_{i, j=1}^k q_{ij}^2 \Big\} \cdot \int_{(\R^n)^k} \prod_{i=1}^k \delta \Big( \langle \bu,\bsigma_i\rangle - n q_{0i} \Big) \prod_{1 \le i &lt; j\le k} \delta\Big( \langle\bsigma_i, \bsigma_j\rangle - n q_{ij} \Big) \prod_{i \in [k]} \nu_0(\de \bsigma_i) \\
=&amp; \int \de \bQ  \exp\Big\{\beta n \lambda \sum_{i = 1}^k  q_{0i}^2 + n \beta^2  \sum_{i, j=1}^k q_{ij}^2 + n H_n(\bQ) \Big\}, <br />
\end{aligned}
\]
where <script type="math/tex">\bQ \in \R^{(k + 1) \times (k + 1)}</script> is given by Eq. \eqref{eqn:def_Q} and
\[
\begin{aligned}
H_n(\bQ) =&amp; \frac{1}{n} \log \int_{(\R^n)^k}  \prod_{i=1}^k \delta \Big( \langle \bu,\bsigma_i\rangle - n q_{0i} \Big) \prod_{1 \le i &lt; j\le k} \delta\Big( \langle\bsigma_i, \bsigma_j\rangle - n q_{ij} \Big) \prod_{i \in [k]} \nu_0(\de \bsigma_i) \\
=&amp; \frac{1}{n} \log \int_{(\R^n)^{k+1}} \prod_{0 \le i &lt; j\le k} \delta\Big( \langle\bsigma_i, \bsigma_j\rangle - n q_{ij} \Big) \prod_{0 \le i \le k} \nu_0( \bsigma_i), 
\end{aligned}
\]
which serves as the rate function of the random matrix <script type="math/tex">\bSigma = (\langle \bsigma_i, \bsigma_j \rangle / n)_{0 \le i, j \le k} \in \R^{(k+1) \times (k+1)}</script> when <script type="math/tex">(\bsigma_i)_{0 \le i \le k} \sim_{iid} \Unif(\S^{n-1})</script>. This rate function can be found on standard textbook
\[
\lim_{n \to \infty} H_n(\bQ) = \frac{1}{2}\log \det(\bQ). 
\]
Here we give a heuristic argument to calculate the entropy term <script type="math/tex">H_n(\bQ)</script>, (here the argument is handwaving, but can be made rigorous)</p>
<p>
\[
\begin{aligned}
H_n(\bQ) \stackrel{\cdot}{=}&amp; \frac{1}{n} \P_{(\bsigma_i)_{i \in [k]} \sim \Unif(\S^{n-1})} \Big(  \big(\langle \bsigma_i, \bsigma_j \rangle / n \big)_{i, j \in [k]}\approx \bQ  \Big)\\\
\stackrel{\cdot}{=}&amp; \inf_{\lambda_{ij}} \frac{1}{n} \log  \int_{(\R^n)^{k+1}} \prod_{0 \le i \le j\le k} \exp\Big\{ - \lambda_{ij}\langle\bsigma_i, \bsigma_j\rangle / 2 + n q_{ij} \lambda_{ij} / 2 \Big\} \prod_{0 \le i \le k} \de \bsigma_i + \const\\\
=&amp; \inf_{\lambda_{ij}} \log  \int_{\R^{k+1}} \prod_{0 \le i &lt; j\le k} \exp\Big\{ -  \lambda_{ij} \sigma_i\sigma_j / 2 + q_{ij} \lambda_{ij}/2 \Big\} \prod_{0 \le i \le k} \de \bsigma_i + \const\\\
=&amp; \inf_{\bLambda = (\lambda_{ij})_{0 \le i \le j \le k}} \Big[ \langle \bQ, \bLambda\rangle / 2 - \log(\det(\bLambda))/2  \Big] + \const\\\
\stackrel{\cdot}{=}&amp; \frac{1}{2} \log \det(\bQ),
\end{aligned}
\]
</p>
<p>where we use the fact that
\[
\lim_{n \to \infty} \frac{1}{n} \log \P( X_n \approx x) = \inf_{\lambda} \lim_{n \to \infty} \frac{1}{n} \log \E \Big[\exp\{ \lambda (X_n - x) \} \Big]. 
\]
This gives Eq. \eqref{eqn:moments_result}.</p>

<h3 id="sec:rsa">4.2. The small <script type="math/tex">k</script> limit: replica symmetric ansatz</h3>

<h4 id="421-replica-symmetric-ansatz">4.2.1. Replica symmetric ansatz</h4>

<p>Next we calculate 
\[
\Psi(\beta, \lambda) = \lim_{k \to 0} S(\beta, \lambda, k)/k. 
\]
We gave the expression of <script type="math/tex">S</script> when <script type="math/tex">k</script> is integral (c.f. Eq. \eqref{eqn:S_function}), where <script type="math/tex">k</script> serves as the dimension of variable <script type="math/tex">\bQ</script> in the variational formula. However, to take the limit <script type="math/tex">k \to 0</script>, we need the expression for <script type="math/tex">S</script> when <script type="math/tex">k</script> taking real numbers. This is the difficulty to calculate the small <script type="math/tex">k</script> limit using Eq. \eqref{eqn:S_function}.</p>

<p>The physicists’ trick is to plug in an ansatz for the matrix <script type="math/tex">\bQ</script> to simplify the expression of <script type="math/tex">S</script>. Note <script type="math/tex">U(\bQ)</script> defined in Eq. \eqref{eqn:U_function} has some symmetry: if we permute the <script type="math/tex">1</script> to <script type="math/tex">k</script> rows of <script type="math/tex">\bQ</script>, and perform the same permutation to its columns, the function <script type="math/tex">U</script> stays the same. This motivates us to assume the “replica symmetric ansatz”: 
\[
\bQ = \begin{bmatrix}
1&amp; \mu &amp; \ldots &amp; \mu\\
\mu &amp; 1 &amp; \ldots &amp; q\\
\ldots &amp; \ldots &amp; \ldots &amp; \ldots\\
\mu &amp; q &amp; \ldots &amp; 1\\
\end{bmatrix}.
\]
Plugging this ansats into Eq. \eqref{eqn:U_function}, and simplifying the log determinant term using the matrix determinant lemma 
\[
\begin{aligned}
&amp;\log(\det(\bQ)) = \log \det( (1-q) \id_k + (q - \mu^2) \ones \ones^\sT) \\
=&amp;  \log \det( \id_k + [(q - \mu^2)/(1-q) ] \ones \ones^\sT) + k \log(1-q)\\
=&amp; \log(1 + k [(q - \mu^2)/(1-q) ]) + k \log(1-q)\\
=&amp; \log\Big( 1 - \frac{\mu^2 k}{1 + (k - 1)q} \Big) + \log \Big(1 + \frac{kq}{1-q} \Big) + k \log(1 - q),
\end{aligned}
\]
we get
\[
U(\bQ) = \beta \lambda k \mu^2 + \beta^2 k + \beta^2 k (k-1) q^2 + \frac{1}{2} \Big[ \log\Big( 1 - \frac{\mu^2 k}{1 + (k - 1)q} \Big) + \log \Big(1 + \frac{kq}{1-q} \Big) + k \log(1 - q) \Big] \equiv U(\mu, q, k). 
\]</p>

<p>Assuming that the maximum of <script type="math/tex">U</script> is taken at a “replica symmetric” form of <script type="math/tex">\bQ</script>, we get
\[
S(\beta, \lambda, k) \stackrel{\cdot}{=} \sup_{\mu, q} U(\mu, q, k).
\]</p>

<h4 id="422-the-small-k-limit">4.2.2. The small <script type="math/tex">k</script> limit</h4>
<p>Using the replica symmetric ansatz, we gave an expression for <script type="math/tex">S</script> for any <script type="math/tex">k > 0</script>. Next we would like to calculate the <script type="math/tex">k \to 0</script> limit (recall the definition of <script type="math/tex">\Psi</script> given in Eq. \eqref{eqn:defPsi})
\[
\Psi(\beta, \lambda) = \lim_{k \to 0} S(\beta, \lambda, k) / k = \lim_{k \to 0} \sup_{\mu, q} \frac{1}{k} U(\mu, q, k). 
\]
Define 
\[
u(\mu, q) \equiv \lim_{k \to 0} \frac{1}{k} U(\mu, q, k) = \beta \lambda \mu^2  + \beta^2 (1 - q^2) - \frac{\mu^2}{2(1 - q)} + \frac{q}{2 (1 - q)} + \frac{1}{2}\log(1 - q). 
\]
We need to exchange the operation <script type="math/tex">\lim_{k \to 0}</script> and <script type="math/tex">\sup_{\mu, q}</script> in some way. Another heuristic physicists’ convention comes here: the <script type="math/tex">\lim_{k \to 0} \sup_{\mu, q}</script> operation becomes the <script type="math/tex">\ext_{\mu, q} \lim_{k \to 0}</script> operation, where <script type="math/tex">\ext_{\bx} f(\bx)</script> is a set defined as
\[
\ext_{\bx} f(\bx) = \Big\{ f(\bx_\star): \nabla f(\bx) = \bzero\Big\}. 
\]
Using this convention, we get
\[
\Psi(\beta, \lambda) =  \lim_{k \to 0} S(\beta, \lambda, k) / k \stackrel{\cdot}{=} \ext_{\mu, q}u(\mu, q),
\]</p>

<p>Next, we calculate all the stationary points of <script type="math/tex">u</script>, and collect the value of <script type="math/tex">u</script> evaluated at these stationary points.</p>

<p>By basic calculus, we have 
\[
\begin{aligned}
\partial_\mu u(\mu, q) =&amp; 2 \Big( \beta \lambda - \frac{1}{2 (1-q)} \Big) \mu, \\
\partial_q u(\mu, q) =&amp; -2 \beta^2 q -  \frac{\mu^2}{2(1 - q)^2} + \frac{1}{2 (1 - q)^2}.
\end{aligned}
\]
When <script type="math/tex">\beta > 1</script>, by basic algebra, we find two solutions of <script type="math/tex">\nabla u = \bzero</script>.</p>

<ul>
  <li>One branch of solution of <script type="math/tex">\nabla u = \bzero</script> gives 
  \[
  \begin{aligned}
  \mu_1 =&amp; 0, \\
  q_1 =&amp; 1 - \frac{1}{2 \beta}.
  \end{aligned}
  \]
  At this solution, we have 
  \[
  \Psi_1(\beta, \lambda) = u(\mu_1, q_1) = 2 \beta - \frac{3}{4} - \frac{1}{2} \log(2 \beta).
  \]</li>
  <li>Another branch of solution of <script type="math/tex">\nabla u = \bzero</script> gives 
  \[
  \begin{aligned}
  \mu_2 =&amp; \Big( \Big(1 - \frac{1}{\lambda^2}\Big) \Big(1 - \frac{1}{2 \beta \lambda}\Big)  \Big)^{1/2},\\
  q_2 =&amp; 1 - \frac{1}{2 \lambda \beta},
  \end{aligned}
  \]
  At this solution, we have
  \[
  \Psi_2(\beta, \lambda) = u(\mu_2, q_2) = \beta\Big( \lambda + \frac{1}{\lambda}\Big) - \Big( \frac{1}{4 \lambda^2} + \frac{1}{2} \Big) - \frac{1}{2}\log(2 \lambda \beta). 
  \]</li>
</ul>

<h3 id="43-the-large-beta-limit">4.3. The large <script type="math/tex">\beta</script> limit</h3>

<p>Recall the definition of <script type="math/tex">\vphi</script> in Eq. \eqref{eqn:defvphi}. Using the first branch <script type="math/tex">\Psi_1</script>, we have 
\[
\vphi_1(\lambda) = \lim_{\beta \to \infty} \frac{1}{\beta} \Psi_1(\beta, \lambda) = 2. 
\]
Using the second branch <script type="math/tex">\Psi_2</script>, we have 
\[
\vphi_2(\lambda) = \lim_{\beta \to \infty} \frac{1}{\beta} \Psi_2(\beta, \lambda) = \lambda + \frac{1}{\lambda}. 
\]</p>

<h3 id="44-select-a-branch">4.4. Select a branch</h3>

<p>There are two branches of solutions. We need to select the branch that makes sense: <script type="math/tex">\vphi</script> stands for <script type="math/tex">\lim_{n\to \infty}\E[\lambda_\max(\bA)]</script>, hence it should be non-decreasing in <script type="math/tex">\lambda</script>, and we must have <script type="math/tex">\lim_{\lambda \to \infty} \vphi(\lambda) = \infty</script>.</p>
<ul>
  <li>When <script type="math/tex">\lambda\le 1</script>, the branch <script type="math/tex">\vphi_2(\lambda) = \lambda + 1/\lambda</script> is decreasing. Hence, we should select the branch <script type="math/tex">\vphi_1(\lambda) = 2</script>.</li>
  <li>When <script type="math/tex">\lambda \ge 1</script>, the branch <script type="math/tex">\vphi_1(\lambda) = 2</script> stays constant. Hence, we should select the branch <script type="math/tex">\vphi_2(\lambda) = \lambda + 1/\lambda</script>.</li>
</ul>

<p>The discussion above gives the following prediction 
\[
\lim_{n \to \infty} \E[\lambda_{\max}(\bA)]= \vphi(\lambda) = \begin{cases}
2, ~~~&amp; \lambda \le 1, \\
\lambda + \frac{1}{\lambda}, ~~~&amp; \lambda &gt; 1.
\end{cases}
\]
This turns out to be the correct answer!</p>

<h2 id="5-summary">5. Summary</h2>

<p>In summary, we calculated the largest eigenvalue of the spiked GOE matrix <script type="math/tex">\bA</script>. After using the free energy trick and the replica trick, the problem becomes calculating three limits sequentially: the large <script type="math/tex">n</script> limit, the small <script type="math/tex">k</script> limit, and the large <script type="math/tex">\beta</script> limit. The large <script type="math/tex">n</script> limit calculation is rigorous, but is also complicated. The small <script type="math/tex">k</script> limit calculation is non-rigorous: we pluged in the replica symmetric ansatz, and changed the <script type="math/tex">\lim_{k \to 0} \inf_{\mu, q}</script> operation to the <script type="math/tex">\ext_{\mu, q} \lim_{k \to 0}</script> operation. The large <script type="math/tex">\beta</script> limit calculation is straightforward. Finally, we get two branches of solutions. We use the properties of the largest eigenvalue of <script type="math/tex">\bA</script> to select the branch when <script type="math/tex">\lambda</script> is in different regime.</p>


 </div>


<!--   <div class="prev-next">
  
    <a class="half prev" href="/jekyll/update/2019/07/19/welcome-to-jekyll.html">&laquo; The topics of this Blog</a>
  
  
</div>
 -->
<!-- 

  
 -->
</article>

      </div>
    </main><footer class="site-footer h-card">
  <data class="u-url" href="/"></data>

  <div class="wrapper">

    <div class="footer-col-wrapper">
      <div class="footer-col one-half">
      <h2 class="footer-heading">Entropic Flow</h2>
        <ul class="contact-list">
          <li class="p-name">Song Mei</li><li><a class="u-email" href="mailto:songmei@stanford.edu">songmei@stanford.edu</a></li></ul>
      </div>

      <div class="footer-col one-half">
        <p>A research blog on machine learning and statistics. 
</p>
      </div>

      <div class="social-links"><ul class="social-media-list"></ul>
</div>
    </div>

  </div>

</footer>
</body>

</html>
